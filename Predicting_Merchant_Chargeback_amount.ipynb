{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyKzhTGontRy8ghfGRR59/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurabhiInamdar/DS_Projects/blob/main/Predicting_Merchant_Chargeback_amount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmiCctAxjXyu"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "# Perform data preprocessing, feature engineering, and handle missing values as needed\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X = data.drop(columns=['chargeback_amount'])\n",
        "y = data['chargeback_amount']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Choose individual base models\n",
        "linear_model = LinearRegression()\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "gbm_model = GradientBoostingRegressor(random_state=42)\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "adaboost_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 4: Train the base models\n",
        "linear_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the base models\n",
        "def evaluate_model(model, X, y_true):\n",
        "    y_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    n = X.shape[0]  # Number of samples\n",
        "    k = X.shape[1]  # Number of predictors (features)\n",
        "    adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - k - 1)\n",
        "    medae = median_absolute_error(y_true, y_pred)\n",
        "    evs = explained_variance_score(y_true, y_pred)\n",
        "    return mse, rmse, mae, r2, adj_r2, medae, evs\n",
        "\n",
        "lr_mse, lr_rmse, lr_mae, lr_r2, lr_adj_r2,lr_medae, lr_evs = evaluate_model(linear_model, X_test, y_test)\n",
        "rf_mse, rf_rmse, rf_mae, rf_r2, rf_adj_r2,rf_medae, rf_evs = evaluate_model(rf_model, X_test, y_test)\n",
        "gbm_mse, gbm_rmse, gbm_mae, gbm_r2, gbm_adj_r2,gbm_medae, gbm_evs = evaluate_model(gbm_model, X_test, y_test)\n",
        "# Similarly for xgb_mpddel, adaboost_model\n",
        "\n",
        "print(\"Linear Regression:\")\n",
        "print(f\"MSE: {lr_mse}\")\n",
        "print(f\"RMSE: {lr_rmse}\")\n",
        "print(f\"MAE: {lr_mae}\")\n",
        "print(f\"R2 Score: {lr_r2}\")\n",
        "print(f\"Adjusted R2 Score: {lr_adj_r2}\")\n",
        "print(f\"Median Absolute Error: {lr_medae}\")\n",
        "print(f\"Explained Variance Score: {lr_evs}\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"MSE: {rf_mse}\")\n",
        "print(f\"RMSE: {rf_rmse}\")\n",
        "print(f\"MAE: {rf_mae}\")\n",
        "print(f\"R2 Score: {rf_r2}\")\n",
        "print(f\"Adjusted R2 Score: {rf_adj_r2}\")\n",
        "print(f\"Median Absolute Error: {rf_medae}\")\n",
        "print(f\"Explained Variance Score: {rf_evs}\")\n",
        "\n",
        "print(\"\\nGradient Boosting:\")\n",
        "print(f\"MSE: {gbm_mse}\")\n",
        "print(f\"RMSE: {gbm_rmse}\")\n",
        "print(f\"MAE: {gbm_mae}\")\n",
        "print(f\"R2 Score: {gbm_r2}\")\n",
        "print(f\"Adjusted R2 Score: {gbm_adj_r2}\")\n",
        "print(f\"Median Absolute Error: {gbm_medae}\")\n",
        "print(f\"Explained Variance Score: {gbm_evs}\")\n",
        "\n",
        "# Similarly for xgb_mpddel, adaboost_model\n",
        "\n",
        "# Step 6: Create the Ensemble model using VotingRegressor\n",
        "ensemble_model = VotingRegressor(estimators=[\n",
        "    ('linear', linear_model),\n",
        "    ('rf', rf_model),\n",
        "    ('gbm', gbm_model),\n",
        "    ('xgb', xgb_model),\n",
        "    ('adb', adaboost_model)\n",
        "])\n",
        "\n",
        "# Step 7: Train the Ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the Ensemble model\n",
        "ensemble_mse, ensemble_rmse, ensemble_mae, ensemble_r2, ensemble_adj_r2, ensemble_medae, ensemble_evs = evaluate_model(ensemble_model, X_test, y_test)\n",
        "\n",
        "print(\"\\nEnsemble Model:\")\n",
        "print(f\"MSE: {ensemble_mse}\")\n",
        "print(f\"RMSE: {ensemble_rmse}\")\n",
        "print(f\"MAE: {ensemble_mae}\")\n",
        "print(f\"R2 Score: {ensemble_r2}\")\n",
        "print(f\"Adjusted R2 Score: {ensemble_adj_r2}\")\n",
        "print(f\"Median Absolute Error: {ensemble_medae}\")\n",
        "print(f\"Explained Variance Score: {ensemble_evs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "# Perform data preprocessing, feature engineering, and handle missing values as needed\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X = data.drop(columns=['chargeback_amount'])\n",
        "y = data['chargeback_amount']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create the base models\n",
        "linear_model = LinearRegression()\n",
        "adaboost_model = AdaBoostRegressor(random_state=42)\n",
        "gbm_model = GradientBoostingRegressor(random_state=42)\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# Step 4: Define the hyperparameter grids for Grid Search\n",
        "\n",
        "# For GBM\n",
        "gbm_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# For Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# For AdaBoost\n",
        "adaboost_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# For XGBoost\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Step 5: Perform Grid Search with cross-validation for each model\n",
        "base_models = {\n",
        "    'Linear Regression': (linear_model, None),  # No hyperparameters to tune for Linear Regression\n",
        "    'Gradient Boosting': (gbm_model, gbm_param_grid),\n",
        "    'Random Forest': (rf_model, rf_param_grid),\n",
        "    'AdaBoost': (adaboost_model, adaboost_param_grid),\n",
        "    'XGBoost': (xgb_model, xgb_param_grid),\n",
        "}\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for model_name, (model, param_grid) in base_models.items():\n",
        "    if param_grid is not None:\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_params = grid_search.best_params_\n",
        "        best_model = grid_search.best_estimator_\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_params = \"No Hyperparameters to Tune\"\n",
        "        best_model = model\n",
        "\n",
        "    best_models[model_name] = best_model\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"Best Hyperparameters: {best_params}\")\n",
        "    print(f\"MSE: {mse}\")\n",
        "    print(f\"R2 Score: {r2}\\n\")\n",
        "\n",
        "# Step 6: Create the Ensemble model using VotingRegressor\n",
        "ensemble_model = VotingRegressor(list(best_models.items()))\n",
        "\n",
        "# Step 7: Train the Ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the Ensemble model\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Ensemble Model:\")\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"R2 Score: {r2}\")\n"
      ],
      "metadata": {
        "id": "Xb2B9Zg5tgsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Load and preprocess the dataset (Replace 'your_data.csv' with your actual dataset file)\n",
        "data = pd.read_csv('your_data.csv')\n",
        "# Perform data preprocessing, feature engineering, and handle missing values as needed\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X = data.drop(columns=['amount'])  # Features\n",
        "y = data['amount']  # Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Perform PCA on the training set\n",
        "pca = PCA(n_components=2)  # Set the number of principal components to retain\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# Step 4: Transform the testing set using the same PCA transformation\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Step 5: Create and train the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model on the testing set (Optional, for evaluation purposes)\n",
        "y_pred_test = linear_model.predict(X_test_pca)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(\"Evaluation on the Testing Set:\")\n",
        "print(f\"MSE: {mse_test}\")\n",
        "print(f\"R2 Score: {r2_test}\")\n",
        "\n",
        "# Step 7: Predict amounts for new records\n",
        "new_records = pd.read_csv('new_records.csv')  # Replace 'new_records.csv' with your new data file\n",
        "# Perform the same preprocessing steps on the new records as done on the training data\n",
        "\n",
        "# Step 8: Perform PCA on the new records\n",
        "X_new_records = new_records.drop(columns=['amount'])\n",
        "X_new_records_pca = pca.transform(X_new_records)\n",
        "\n",
        "# Step 9: Make predictions using the Linear Regression model\n",
        "predicted_amounts = linear_model.predict(X_new_records_pca)\n",
        "\n",
        "# Add the predicted amounts to the new_records DataFrame\n",
        "new_records['predicted_amount'] = predicted_amounts\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nPredicted Amounts for New Records:\")\n",
        "print(new_records[['predicted_amount']])\n"
      ],
      "metadata": {
        "id": "B_fYn8FR7lMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plotting the cumulative explained variance with the \"elbow\" point\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance for PCA')\n",
        "plt.grid(True)\n",
        "\n",
        "# Find the \"elbow\" point where the variance levels off\n",
        "elbow_point = np.argmax(np.diff(cumulative_variance) < 0.005) + 1\n",
        "plt.axvline(x=elbow_point, color='red', linestyle='--', label=f'Elbow Point: {elbow_point} Components')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mM326qt--LTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Assuming you have the dataset in a pandas DataFrame\n",
        "data = pd.read_csv('your_data.csv')\n",
        "X = data.drop(columns=['target'])  # Features\n",
        "y = data['target']  # Target variable\n",
        "\n",
        "# Create the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform k-fold cross-validation with k=5 (you can change the value of k as needed)\n",
        "k_folds = 5\n",
        "scores = cross_val_score(model, X, y, cv=k_folds, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Convert negative mean squared error to positive Root Mean Squared Error (RMSE)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "# Print the RMSE for each fold and the average RMSE\n",
        "print(\"Root Mean Squared Error (RMSE) for each fold:\")\n",
        "for fold, rmse in enumerate(rmse_scores, start=1):\n",
        "    print(f\"Fold {fold}: {rmse}\")\n",
        "\n",
        "print(\"\\nAverage RMSE:\", np.mean(rmse_scores))"
      ],
      "metadata": {
        "id": "7ZXNZ8_BAwMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}